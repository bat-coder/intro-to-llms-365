{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93df7da0",
   "metadata": {},
   "source": [
    "## HuggingFace Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7347cb4a",
   "metadata": {},
   "source": [
    "# HuggingFace Transformers Tutorial Documentation\n",
    "\n",
    "This notebook demonstrates key concepts and usage of the HuggingFace Transformers library. Here's a section-by-section breakdown:\n",
    "\n",
    "## 1. Basic Pipeline Usage\n",
    "The notebook starts with simple pipeline examples:\n",
    "- Sentiment analysis using BERT\n",
    "- Named Entity Recognition (NER) using a specialized BERT model\n",
    "- Zero-shot classification using BART\n",
    "- Text generation using OPT-1.3B\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f8106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Basic Pipeline Usage\n",
    "# Import the main pipeline interface from transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create a sentiment analysis pipeline using BERT\n",
    "# This will automatically download the model on first use\n",
    "sentiment_classifier = pipeline(task=\"sentiment-analysis\", model=\"bert-base-uncased\")\n",
    "\n",
    "# Test the sentiment classifier with a sample input\n",
    "sentiment_classifier(inputs=\"I'm so excited to be learning about large language models\")\n",
    "\n",
    "# Create Named Entity Recognition pipeline using a specialized BERT model\n",
    "ner = pipeline(task=\"ner\", model = \"dslim/bert-base-NER\")\n",
    "\n",
    "# Set up zero-shot classification pipeline using BART\n",
    "zeroshot_classifier = pipeline(task=\"zero-shot-classification\", model = \"facebook/bart-large-mnli\")\n",
    "\n",
    "# Define test inputs for zero-shot classification\n",
    "sequence_to_classify = \"one day I will see the world\"\n",
    "candidate_labels = ['travel', 'cooking', 'dancing']\n",
    "\n",
    "# Text generation pipeline using OPT-1.3B\n",
    "# Using bfloat16 for memory efficiency and auto device mapping\n",
    "zeroshot_classifier(sequence_to_classify, candidate_labels)\n",
    "import torch\n",
    "pipe = pipeline(model=\"facebook/opt-1.3b\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6bc715",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. Tokenizer Operations\n",
    "Demonstrates tokenizer functionality using BERT and XLNet models:\n",
    "- Loading pre-trained tokenizers\n",
    "- Converting text to tokens\n",
    "- Converting tokens to IDs\n",
    "- Decoding tokens back to text\n",
    "- Comparing different tokenizer behaviors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2618508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Tokenizer Operations\n",
    "# Import AutoTokenizer for automatic tokenizer loading\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "model = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "# Example text for tokenization\n",
    "sentence = \"I'm so excited to be learning about large language models\"\n",
    "\n",
    "# Convert text to tokens and display\n",
    "input_ids = tokenizer(sentence)\n",
    "print(input_ids)\n",
    "\n",
    "# Tokenize the text and show individual tokens\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)\n",
    "\n",
    "# Convert tokens to their numerical IDs\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(token_ids)\n",
    "\n",
    "# Decode tokens back to text\n",
    "decoded_ids = tokenizer.decode(token_ids)\n",
    "print(decoded_ids)\n",
    "\n",
    "# Compare with XLNet tokenizer\n",
    "model2 = \"xlnet-base-cased\"\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(model2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ba0b96",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3. PyTorch Integration\n",
    "Shows how to:\n",
    "- Use models with PyTorch\n",
    "- Perform inference using pre-trained models\n",
    "- Handle tensors and model outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998a510d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. PyTorch Integration\n",
    "# Import required modules for PyTorch integration\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Initialize DistilBERT tokenizer and model for sentiment analysis\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Convert input to PyTorch tensors\n",
    "input_ids_pt = tokenizer(sentence, return_tensors =\"pt\")\n",
    "print(input_ids_pt)\n",
    "\n",
    "# Perform inference without gradient calculation\n",
    "with torch.no_grad():\n",
    "    logits = model(**input_ids_pt).logits\n",
    "\n",
    "# Get prediction from model output\n",
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d15a6f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. Model Management\n",
    "Demonstrates how to:\n",
    "- Save models locally\n",
    "- Load models from local storage\n",
    "- Handle model configurations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92996547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Model Management\n",
    "# Define directory for saving models\n",
    "model_directory = \"my_saved_models\"\n",
    "\n",
    "# Save both tokenizer and model to local directory\n",
    "tokenizer.save_pretrained(model_directory)\n",
    "model.save_pretrained(model_directory)\n",
    "\n",
    "# Load saved model and tokenizer from local directory\n",
    "my_tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "my_model = AutoModelForSequenceClassification.from_pretrained(model_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cb94b5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Requirements\n",
    "- transformers library\n",
    "- PyTorch\n",
    "- Sufficient disk space for model storage\n",
    "\n",
    "## Usage Notes\n",
    "- Models are downloaded on first use\n",
    "- Some operations require significant memory\n",
    "- Consider using GPU acceleration for larger models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
